import streamlit as st
import pandas as pd
import numpy as np
import xgboost as xgb
import pulp
import jpholiday
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from io import BytesIO
from pytrends.request import TrendReq # Googleãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
import time # APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ãŸã‚ã®å¾…æ©Ÿæ™‚é–“

# --- 0. ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
def to_excel(df):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’Excelå½¢å¼ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=True, sheet_name='Sheet1')
    processed_data = output.getvalue()
    return processed_data

# --- Googleãƒˆãƒ¬ãƒ³ãƒ‰é–¢é€£ã®é–¢æ•° (ä¿®æ­£) ---
# ãƒˆãƒ¬ãƒ³ãƒ‰å–å¾—å¯¾è±¡ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å®šç¾© (ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æŒ‡å®šã«åŸºã¥ãæ›´æ–°)
TREND_KEYWORDS = {
    'general_trend': ['å¡¾è¬›å¸« ãƒã‚¤ãƒˆ', 'å¡¾ ãƒã‚¤ãƒˆ'],
    'brand_trend': ['å¡¾è¬›å¸«ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³'],
    'school_trend': [
        'æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼ ãƒã‚¤ãƒˆ', 'æ­¦ç”°å¡¾ ãƒã‚¤ãƒˆ', 'ITTOå€‹åˆ¥æŒ‡å°Žå­¦é™¢ ãƒã‚¤ãƒˆ',
        'ãƒ“ã‚¶ãƒ“ ãƒã‚¤ãƒˆ', 'æ—©ç¨²ã‚¢ã‚« ãƒã‚¤ãƒˆ', 'æ—©ç¨²ã‚¢ã‚« æ±‚äºº', 'å€‹åˆ¥æŒ‡å°Žã‚­ãƒ£ãƒ³ãƒ‘ã‚¹ ãƒã‚¤ãƒˆ',
        'æ­¦ç”°å¡¾ å¿œå‹Ÿ', 'ãƒˆãƒ¼ãƒžã‚¹ ãƒã‚¤ãƒˆ', 'ãƒŠãƒ“å€‹åˆ¥æŒ‡å°Žå­¦é™¢ ã‚¢ãƒ«ãƒã‚¤ãƒˆ',
        'ã‚¢ã‚¯ã‚·ã‚¹ æ±‚äºº', 'ãƒŠãƒ“å€‹åˆ¥æŒ‡å°Žå­¦é™¢ ãƒã‚¤ãƒˆ', 'ITTO ãƒã‚¤ãƒˆ', 'ã‚¢ã‚¯ã‚·ã‚¹ ãƒã‚¤ãƒˆ',
        'ä»£ã€…æœ¨å€‹åˆ¥æŒ‡å°Žå­¦é™¢ ãƒã‚¤ãƒˆ', 'è‡¨æµ·ã‚»ãƒŸãƒŠãƒ¼ ãƒã‚¤ãƒˆ', 'å€‹åˆ¥æŒ‡å°ŽAxis ãƒã‚¤ãƒˆ'
    ],
    'station_trend': [
        'æ±äº¬ å¡¾ æ±‚äºº', 'ç¥žå¥ˆå· å¡¾ æ±‚äºº', 'åƒè‘‰ å¡¾ æ±‚äºº', 'åå¤å±‹ å¡¾ æ±‚äºº',
        'äº¬éƒ½ å¡¾ æ±‚äºº', 'æ±äº¬ å¡¾ ãƒã‚¤ãƒˆ', 'ç¦å³¶ å¡¾ æ±‚äºº', 'å¤§é˜ª å¡¾ æ±‚äºº',
        'æµœæ¾ å¡¾ æ±‚äºº', 'ç›¸æ¨¡åŽŸ å¡¾ æ±‚äºº', 'ã¤ãã° å¡¾ æ±‚äºº', 'å…«çŽ‹å­ å¡¾ æ±‚äºº',
        'å¤§å®® å¡¾ ãƒã‚¤ãƒˆ', 'åƒè‘‰ å¡¾ ãƒã‚¤ãƒˆ', 'æ±å¤§å®® å¡¾ ãƒã‚¤ãƒˆ', 'ç¦å³¶ å¡¾ ãƒã‚¤ãƒˆ',
        'å‰ç¥¥å¯º å¡¾ ãƒã‚¤ãƒˆ', 'æ¹˜å—å° å¡¾ ãƒã‚¤ãƒˆ', 'æ­¦è”µå°æ‰ å¡¾ ãƒã‚¤ãƒˆ', 'æ´¥ç”°æ²¼ å¡¾ ãƒã‚¤ãƒˆ'
    ]
}

# --- ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šæ•°ã¨ã—ã¦å®šç¾© (ä¿®æ­£) ---
FEATURES = ['cost', 'log_cost', 'weekday', 'month', 'week', 'is_holiday'] + list(TREND_KEYWORDS.keys())


@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def fetch_and_prepare_trends_data(start_date, end_date):
    """æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã®Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™"""
    pytrends = TrendReq(hl='ja-JP', tz=360) # æ—¥æœ¬èªžã€æ—¥æœ¬æ™‚é–“ã§è¨­å®š
    
    all_dates = pd.date_range(start=start_date, end=end_date, freq='D')
    trends_df = pd.DataFrame(index=all_dates)

    for category, kw_list in TREND_KEYWORDS.items():
        try:
            # pytrendsã¯å†…éƒ¨ã§5ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«åˆ†å‰²ã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹
            pytrends.build_payload(kw_list, cat=0, timeframe=f'{start_date.strftime("%Y-%m-%d")} {end_date.strftime("%Y-%m-%d")}', geo='JP')
            time.sleep(1) # APIã¸ã®è² è·ã‚’è»½æ¸›
            
            interest_over_time_df = pytrends.interest_over_time()

            if not interest_over_time_df.empty and 'isPartial' in interest_over_time_df.columns:
                trends_df[category] = interest_over_time_df.drop(columns='isPartial').mean(axis=1)
        except Exception as e:
            # st.warning(f"ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({category}): {e}")
            trends_df[category] = 0

    trends_df = trends_df.resample('D').ffill()
    trends_df = trends_df.bfill()
    trends_df = trends_df.fillna(0)
    
    # ã‚«ãƒ©ãƒ åã‚’ 'date' ã‹ã‚‰ 'æ—¥' ã«å¤‰æ›´
    return trends_df.reset_index().rename(columns={'index': 'æ—¥'})


# --- 1. ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•° (ä¿®æ­£) ---
@st.cache_data
def preprocess_data(uploaded_file, column_mapping, training_start_date, training_end_date):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰å‡¦ç†ã—ã€ç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹"""
    if uploaded_file is None: return None, "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", None
    
    df = None
    for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'sjis']:
        try:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding=encoding, header=None, dtype=str)
            break
        except Exception:
            continue
    
    if df is None: return None, "ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¯¾å¿œã™ã‚‹æ–‡å­—ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", None

    header_row_index = -1
    for i, row in df.iterrows():
        if row.astype(str).str.contains('ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å').any():
            header_row_index = i
            break
            
    if header_row_index == -1:
        st.session_state.column_mapping_required = True
        st.session_state.raw_df_columns = df.iloc[0].tolist()
        return None, "ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ ('ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å') ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚", None

    new_header = df.iloc[header_row_index]
    df = df[header_row_index + 1:]
    df.columns = new_header
    
    total_row_index = df[df.iloc[:, 0].astype(str).str.contains('åˆè¨ˆ', na=False)].index
    if not total_row_index.empty:
        df = df.loc[:total_row_index[0]-1]

    df = df.rename(columns=column_mapping)
    
    # å¿…é ˆåˆ—ã‚’ 'date' ã‹ã‚‰ 'æ—¥' ã«å¤‰æ›´
    required_cols = ['æ—¥', 'campaign_name', 'cost', 'conversions']
    if not all(col in df.columns for col in required_cols):
        return None, f"å¿…è¦ãªåˆ— {required_cols} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—ã®ãƒžãƒƒãƒ”ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", None
        
    df_selected = df[required_cols].copy()
    
    try:
        # ãƒ‡ãƒ¼ã‚¿åž‹å¤‰æ›ã®å¯¾è±¡åˆ—ã‚’ 'æ—¥' ã«å¤‰æ›´
        df_selected['æ—¥'] = pd.to_datetime(df_selected['æ—¥'])
        df_selected['cost'] = pd.to_numeric(df_selected['cost'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
        df_selected['conversions'] = pd.to_numeric(df_selected['conversions'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
    except Exception as e:
        return None, f"ãƒ‡ãƒ¼ã‚¿åž‹å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚CSVã®ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", None

    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®å‚ç…§åˆ—ã‚’ 'æ—¥' ã«å¤‰æ›´
    df_selected['weekday'] = df_selected['æ—¥'].dt.weekday
    df_selected['month'] = df_selected['æ—¥'].dt.month
    df_selected['week'] = df_selected['æ—¥'].dt.isocalendar().week.astype(int)
    df_selected['is_holiday'] = df_selected['æ—¥'].apply(lambda x: 1 if jpholiday.is_holiday(x) else 0)
    df_selected['log_cost'] = np.log1p(df_selected['cost'])

    # --- Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒžãƒ¼ã‚¸ (çµåˆã‚­ãƒ¼ã‚’ 'æ—¥' ã«å¤‰æ›´) ---
    try:
        trends_df = fetch_and_prepare_trends_data(training_start_date, training_end_date)
        df_selected = pd.merge(df_selected, trends_df, on='æ—¥', how='left')
        df_selected[list(TREND_KEYWORDS.keys())] = df_selected[list(TREND_KEYWORDS.keys())].fillna(0)
    except Exception as e:
        return None, f"Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¾ãŸã¯ãƒžãƒ¼ã‚¸ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", None
    # --- ã“ã“ã¾ã§ä¿®æ­£ ---

    # å­¦ç¿’æœŸé–“ã§ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (å‚ç…§åˆ—ã‚’ 'æ—¥' ã«å¤‰æ›´)
    training_df = df_selected[(df_selected['æ—¥'] >= pd.to_datetime(training_start_date)) & (df_selected['æ—¥'] <= pd.to_datetime(training_end_date))]
    
    if training_df.empty:
        return None, "æŒ‡å®šã•ã‚ŒãŸå­¦ç¿’æœŸé–“ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", None

    return training_df.sort_values(by='æ—¥').reset_index(drop=True), None, trends_df

# --- 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–¢æ•° (ä¿®æ­£) ---
@st.cache_data
def train_models(_df):
    """ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã”ã¨ã«XGBoostãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹"""
    models = {}
    # st.session_stateã¸ã®ä¾å­˜ã‚’å‰Šé™¤
    
    campaign_names = _df['campaign_name'].unique()
    # æ—¥ä»˜ã®å‚ç…§åˆ—ã‚’ 'æ—¥' ã«å¤‰æ›´
    latest_date = _df['æ—¥'].max()
    
    for campaign in campaign_names:
        campaign_df = _df[_df['campaign_name'] == campaign].copy()
        if len(campaign_df) < 10: continue

        # æ—¥ä»˜ã®å‚ç…§åˆ—ã‚’ 'æ—¥' ã«å¤‰æ›´
        recency_in_days = (latest_date - campaign_df['æ—¥']).dt.days
        sample_weights = np.where(recency_in_days <= 30, 3.0, 1.0)
        
        # ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šæ•° FEATURES ã‚’ä½¿ç”¨
        X = campaign_df[FEATURES]
        y = campaign_df['conversions']
        
        model = xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=100,
            learning_rate=0.1,
            max_depth=3,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X, y, sample_weight=sample_weights)
        models[campaign] = model
        
    return models

# --- 3. æœ€é©åŒ–é–¢æ•° (ä¿®æ­£) ---
def optimize_budget_allocation(total_budget, models, features_today, campaign_max_budgets):
    """æ•°ç†æœ€é©åŒ–ã‚’ç”¨ã„ã¦ã€CVã‚’æœ€å¤§åŒ–ã™ã‚‹äºˆç®—é…åˆ†ã‚’è¨ˆç®—ã™ã‚‹"""
    campaign_names = list(models.keys())
    problem = pulp.LpProblem("Budget_Allocation_Problem", pulp.LpMaximize)
    
    step = max(1000, int(total_budget / 100))
    budget_steps = list(range(step, total_budget + 1, step))
    if not budget_steps: return {}, None, {}
    
    choices = pulp.LpVariable.dicts("Choice", (campaign_names, budget_steps), cat='Binary')
    
    predicted_cvs = {}
    # st.session_stateã¸ã®ä¾å­˜ã‚’å‰Šé™¤ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«å®šæ•° FEATURES ã‚’ä½¿ç”¨
    for campaign in campaign_names:
        predicted_cvs[campaign] = {}
        pred_data_list = []
        for budget in budget_steps:
            pred_data_point = {
                'cost': budget, 
                'log_cost': np.log1p(budget),
                **features_today
            }
            pred_data_list.append(pred_data_point)

        pred_df = pd.DataFrame(pred_data_list)[FEATURES]
        predictions = models[campaign].predict(pred_df)
        for i, budget in enumerate(budget_steps):
            predicted_cvs[campaign][budget] = max(0, predictions[i])

    problem += pulp.lpSum(predicted_cvs[c][b] * choices[c][b] for c in campaign_names for b in budget_steps)
    
    problem += pulp.lpSum(b * choices[c][b] for c in campaign_names for b in budget_steps) <= total_budget
    for c in campaign_names:
        problem += pulp.lpSum(choices[c][b] for b in budget_steps) == 1
    for c in campaign_names:
        problem += pulp.lpSum(b * choices[c][b] for b in budget_steps) <= campaign_max_budgets[c]

    problem.solve(pulp.PULP_CBC_CMD(msg=0))
    
    optimal_allocation, total_predicted_cv, cv_per_campaign = {}, None, {}
    if pulp.LpStatus[problem.status] == "Optimal":
        total_predicted_cv = pulp.value(problem.objective)
        for c in campaign_names:
            for b in budget_steps:
                if choices[c][b].varValue == 1:
                    optimal_allocation[c] = b
                    cv_per_campaign[c] = predicted_cvs[c][b]
                    break
    return optimal_allocation, total_predicted_cv, cv_per_campaign

# --- 4. Streamlit UIã®æ§‹ç¯‰ ---
st.set_page_config(layout="wide", page_title="åºƒå‘Šäºˆç®—é…åˆ† æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼")
st.title('ðŸš€ åºƒå‘Šäºˆç®—é…åˆ† æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ (Googleãƒˆãƒ¬ãƒ³ãƒ‰å¯¾å¿œç‰ˆ)')

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header('âš™ï¸ åŸºæœ¬è¨­å®š')
    uploaded_file = st.file_uploader("â‘  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

    if uploaded_file:
        default_start_date = datetime.now().date() - timedelta(days=90)
        default_end_date = datetime.now().date()
        date_detection_info = "å­¦ç¿’æœŸé–“ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚"

        try:
            uploaded_file.seek(0)
            temp_df_for_dates = None
            possible_date_cols = ['æ—¥', 'æ—¥ä»˜', 'Date', 'Day']
            date_col_name = None
            header_row_index = -1
            found_encoding = None

            for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'sjis']:
                try:
                    uploaded_file.seek(0)
                    header_peek = pd.read_csv(uploaded_file, encoding=encoding, header=None, dtype=str, nrows=20)
                    
                    for i, row in header_peek.iterrows():
                        if 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å' in row.astype(str).values:
                            header_row_index = i
                            for col in possible_date_cols:
                                if col in row.values:
                                    date_col_name = col
                                    break
                            break
                    if date_col_name:
                        found_encoding = encoding
                        break
                except Exception:
                    continue
            
            if date_col_name and found_encoding:
                uploaded_file.seek(0)
                date_series = pd.read_csv(uploaded_file, encoding=found_encoding, header=header_row_index, usecols=[date_col_name], dtype=str).iloc[:, 0]
                dates = pd.to_datetime(date_series, errors='coerce').dropna()
                if not dates.empty:
                    default_start_date = dates.min().date()
                    default_end_date = dates.max().date()
                    date_detection_info = "ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æœŸé–“ã‚’è‡ªå‹•è¨­å®šã—ã¾ã—ãŸã€‚å¤‰æ›´ã‚‚å¯èƒ½ã§ã™ã€‚"
        except Exception:
            pass
        finally:
            uploaded_file.seek(0)

        st.subheader("â‘¡ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æœŸé–“")
        st.info(date_detection_info)
        training_start_date = st.date_input('é–‹å§‹æ—¥', value=default_start_date)
        training_end_date = st.date_input('çµ‚äº†æ—¥', value=default_end_date)
        
        process_button = st.button('ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹', type="primary", use_container_width=True)
    else:
        process_button = False

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
# --- session_state ã®åˆæœŸåŒ– (ä¿®æ­£) ---
if 'column_mapping_required' not in st.session_state:
    st.session_state.column_mapping_required = False
if 'data_processed' not in st.session_state:
    st.session_state.data_processed = False
if 'daily_results' not in st.session_state:
    st.session_state.daily_results = None
if 'optim_period' not in st.session_state:
    st.session_state.optim_period = None
if 'original_data' not in st.session_state:
    st.session_state.original_data = None
if 'trained_models' not in st.session_state:
    st.session_state.trained_models = None
if 'raw_df_columns' not in st.session_state:
    st.session_state.raw_df_columns = []


if uploaded_file is None:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
else:
    # åˆ—åãƒžãƒƒãƒ”ãƒ³ã‚°ã‹ã‚‰ 'æ—¥': 'date' ã‚’å‰Šé™¤
    column_mapping = {
        'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å': 'campaign_name', 'ã‚³ã‚¹ãƒˆ': 'cost', 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°': 'conversions'
    }
    if st.session_state.column_mapping_required:
        with st.expander("âš ï¸ åˆ—åã®ãƒžãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ã§ã™", expanded=True):
            st.warning("CSVã®åˆ—åã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰å¯¾å¿œã™ã‚‹åˆ—åã‚’é¸æŠžã—ã¦ãã ã•ã„ã€‚")
            # ãƒžãƒƒãƒ”ãƒ³ã‚°è¾žæ›¸ã‹ã‚‰ 'æ—¥' ã‚’å‰Šé™¤
            jp_to_en = {'æ—¥': 'æ—¥', 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å': 'campaign_name', 'ã‚³ã‚¹ãƒˆ': 'cost', 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°': 'conversions'}
            raw_cols = st.session_state.raw_df_columns
            user_mapping = {}
            for jp_col, en_col in jp_to_en.items():
                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸æŠžã—ãŸCSVã®åˆ—åã‚’ã‚­ãƒ¼ã€ãƒ—ãƒ­ã‚°ãƒ©ãƒ å†…éƒ¨ã§ä½¿ã†åˆ—å(en_col)ã‚’ãƒãƒªãƒ¥ãƒ¼ã¨ã™ã‚‹
                selected_col = st.selectbox(f"ã€Œ{jp_col}ã€ã«å¯¾å¿œã™ã‚‹åˆ—", [None] + raw_cols)
                if selected_col:
                    user_mapping[selected_col] = en_col
            
            column_mapping = {k: v for k, v in user_mapping.items() if k is not None}


    if process_button:
        with st.spinner('ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œä¸­... (å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™)'):
            data, error_message, trends_for_training = preprocess_data(uploaded_file, column_mapping, training_start_date, training_end_date)
            if error_message:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {error_message}")
                st.session_state.data_processed = False
            else:
                st.session_state.original_data = data
                st.session_state.trained_models = train_models(data)
                st.session_state.data_processed = True
                st.success("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    if st.session_state.data_processed:
        st.header("â‘¢ æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        
        col1, col2 = st.columns(2)
        with col1:
            optim_start_date = st.date_input('æœ€é©åŒ–æœŸé–“ï¼ˆé–‹å§‹æ—¥ï¼‰', value=datetime.now().date() + timedelta(days=1))
        with col2:
            optim_end_date = st.date_input('æœ€é©åŒ–æœŸé–“ï¼ˆçµ‚äº†æ—¥ï¼‰', value=datetime.now().date() + timedelta(days=7))

        if optim_start_date > optim_end_date:
            st.error('ã‚¨ãƒ©ãƒ¼: çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚')
        else:
            st.subheader("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¨­å®š")
            total_daily_budget = st.slider('1æ—¥ã‚ãŸã‚Šã®ç·äºˆç®—ï¼ˆå††ï¼‰', min_value=10000, max_value=500000, step=1000, value=100000)
            
            with st.expander("ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ¥ã®ä¸Šé™äºˆç®—ã‚’è¨­å®šã™ã‚‹ï¼ˆæŽ¨å¥¨ï¼‰"):
                st.info("ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æ¯Žã«æ¶ˆåŒ–å¯èƒ½ãªä¸Šé™äºˆç®—ã‚’ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã§è¨­å®šã™ã‚‹ã¨ã€ã‚ˆã‚Šç¾å®Ÿçš„ãªé…åˆ†ã«ãªã‚Šã¾ã™ã€‚")
                campaign_max_budgets_input = {}
                for campaign in st.session_state.trained_models.keys():
                    campaign_max_budgets_input[campaign] = st.slider(f"ã€{campaign}ã€‘ã®ä¸Šé™äºˆç®—ï¼ˆå††/æ—¥ï¼‰", 0, total_daily_budget, total_daily_budget, 500)

            run_optimization_button = st.button('ðŸš€ ã“ã®è¨­å®šã§æœ€é©åŒ–ã‚’å®Ÿè¡Œã™ã‚‹', type="primary", use_container_width=True)

            if run_optimization_button:
                with st.spinner('æœ€é©åŒ–è¨ˆç®—ã‚’å®Ÿè¡Œä¸­... (ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’è€ƒæ…®ã—ã¾ã™)'):
                    trends_for_optim = fetch_and_prepare_trends_data(optim_start_date, optim_end_date)
                    # çµåˆã‚­ãƒ¼ã‚’ 'æ—¥' ã«ã™ã‚‹ãŸã‚ã€ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ 'æ—¥' ã«è¨­å®š
                    trends_for_optim.set_index('æ—¥', inplace=True)

                    date_range = pd.date_range(optim_start_date, optim_end_date)
                    daily_results = []
                    progress_bar = st.progress(0, text="æœ€é©åŒ–è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")

                    for i, target_date in enumerate(date_range):
                        target_date_ts = pd.Timestamp(target_date)
                        
                        trend_date_to_use = min(target_date_ts, trends_for_optim.index.max())
                        
                        features_for_today = {
                            'weekday': target_date.weekday(),
                            'month': target_date.month, 
                            'week': target_date.isocalendar().week,
                            'is_holiday': 1 if jpholiday.is_holiday(target_date) else 0,
                            **trends_for_optim.loc[trend_date_to_use].to_dict()
                        }

                        optimal_budgets, total_cv, cv_breakdown = optimize_budget_allocation(total_daily_budget, st.session_state.trained_models, features_for_today, campaign_max_budgets_input)
                        daily_results.append({'date': target_date, 'allocation': optimal_budgets, 'cv': total_cv, 'cv_breakdown': cv_breakdown})
                        progress_bar.progress((i + 1) / len(date_range), text=f"æœ€é©åŒ–è¨ˆç®—: {target_date.strftime('%m/%d')}")
                    
                    st.session_state.daily_results = daily_results
                    st.session_state.optim_period = (optim_start_date, optim_end_date)
                    st.success("æœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

    # çµæžœè¡¨ç¤º
    if st.session_state.daily_results:
        st.header(f'ðŸ“Š æœ€é©åŒ–çµæžœï¼ˆ{st.session_state.optim_period[0].strftime("%Y/%m/%d")} ã€œ {st.session_state.optim_period[1].strftime("%Y/%m/%d")}ï¼‰')
        
        daily_results = st.session_state.daily_results
        if not daily_results or not any(res['allocation'] for res in daily_results):
            st.warning("æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è§£æ±ºå¯èƒ½ãªé…åˆ†ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ç·äºˆç®—ã‚„ä¸Šé™äºˆç®—ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.container(border=True):
                st.subheader("ðŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚µãƒžãƒªãƒ¼")
                
                total_allocated_sum = sum(sum(res['allocation'].values()) for res in daily_results if res['allocation'])
                total_predicted_cv_sum = sum(res['cv'] for res in daily_results if res['cv'] is not None)
                total_predicted_cpa = total_allocated_sum / total_predicted_cv_sum if total_predicted_cv_sum > 0 else 0

                past_data = st.session_state.original_data
                past_period_start = st.session_state.optim_period[0] - timedelta(days=len(daily_results))
                past_period_end = st.session_state.optim_period[0] - timedelta(days=1)
                # éŽåŽ»ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜åˆ—ã‚‚ 'æ—¥' ã‚’å‚ç…§
                past_perf_df = past_data[(past_data['æ—¥'] >= pd.to_datetime(past_period_start)) & (past_data['æ—¥'] <= pd.to_datetime(past_period_end))]
                
                past_cost = past_perf_df['cost'].sum()
                past_cv = past_perf_df['conversions'].sum()
                past_cpa = past_cost / past_cv if past_cv > 0 else 0

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric(label="äºˆæ¸¬ é…åˆ†åˆè¨ˆé‡‘é¡", value=f"{total_allocated_sum:,.0f} å††", help="ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æœŸé–“ä¸­ã®åˆè¨ˆäºˆç®—")
                    st.metric(label="éŽåŽ»å®Ÿç¸¾ ã‚³ã‚¹ãƒˆ", value=f"{past_cost:,.0f} å††", delta=f"{total_allocated_sum - past_cost:,.0f} å††", delta_color="inverse", help=f"éŽåŽ»ã®åŒæœŸé–“ ({past_period_start.strftime('%m/%d')}~{past_period_end.strftime('%m/%d')}) ã®å®Ÿç¸¾")
                with col2:
                    st.metric(label="äºˆæ¸¬ ç·CVæ•°", value=f"{total_predicted_cv_sum:.2f} ä»¶", help="ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æœŸé–“ä¸­ã®åˆè¨ˆäºˆæ¸¬CVæ•°")
                    st.metric(label="éŽåŽ»å®Ÿç¸¾ CVæ•°", value=f"{past_cv:,.0f} ä»¶", delta=f"{total_predicted_cv_sum - past_cv:.2f} ä»¶", help="éŽåŽ»ã®åŒæœŸé–“ã®å®Ÿç¸¾")
                with col3:
                    st.metric(label="äºˆæ¸¬ CPA", value=f"{total_predicted_cpa:,.0f} å††", help="æœŸé–“å¹³å‡ã®äºˆæ¸¬CPA")
                    st.metric(label="éŽåŽ»å®Ÿç¸¾ CPA", value=f"{past_cpa:,.0f} å††", delta=f"{total_predicted_cpa - past_cpa:,.0f} å††", delta_color="inverse", help="éŽåŽ»ã®åŒæœŸé–“ã®å®Ÿç¸¾")

            avg_allocations = pd.DataFrame([res['allocation'] for res in daily_results if res['allocation']]).mean()
            avg_cvs = pd.DataFrame([res['cv_breakdown'] for res in daily_results if res['cv_breakdown']]).mean()
            result_df = pd.DataFrame({'1æ—¥ã‚ãŸã‚Šã®å¹³å‡æŽ¨å¥¨äºˆç®—': avg_allocations, '1æ—¥ã‚ãŸã‚Šã®å¹³å‡äºˆæ¸¬CVæ•°': avg_cvs}).fillna(0)
            result_df['äºˆæ¸¬CPA'] = (result_df['1æ—¥ã‚ãŸã‚Šã®å¹³å‡æŽ¨å¥¨äºˆç®—'] / result_df['1æ—¥ã‚ãŸã‚Šã®å¹³å‡äºˆæ¸¬CVæ•°']).replace([np.inf, -np.inf], 0).fillna(0)
            result_df = result_df.round({'1æ—¥ã‚ãŸã‚Šã®å¹³å‡æŽ¨å¥¨äºˆç®—': 0, '1æ—¥ã‚ãŸã‚Šã®å¹³å‡äºˆæ¸¬CVæ•°': 2, 'äºˆæ¸¬CPA': 0}).astype({'1æ—¥ã‚ãŸã‚Šã®å¹³å‡æŽ¨å¥¨äºˆç®—': int, 'äºˆæ¸¬CPA': int})
            
            col1, col2 = st.columns([0.6, 0.4])
            with col1:
                with st.container(border=True):
                    st.subheader("ðŸ“‹ ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ï¼ˆæ—¥å¹³å‡ï¼‰")
                    st.dataframe(result_df, use_container_width=True)
                    st.download_button(
                        label="è©³ç´°çµæžœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Excel)",
                        data=to_excel(result_df),
                        file_name=f"optimization_result_{datetime.now().strftime('%Y%m%d')}.xlsx",
                        mime="application/vnd.ms-excel",
                        use_container_width=True
                    )
            with col2:
                with st.container(border=True):
                    st.subheader("ðŸ° äºˆç®—é…åˆ†æ¯”çŽ‡ï¼ˆæ—¥å¹³å‡ï¼‰")
                    plot_df = result_df[result_df['1æ—¥ã‚ãŸã‚Šã®å¹³å‡æŽ¨å¥¨äºˆç®—'] > 0]
                    if not plot_df.empty:
                        fig = px.pie(plot_df, values='1æ—¥ã‚ãŸã‚Šã®å¹³å‡æŽ¨å¥¨äºˆç®—', names=plot_df.index, hole=.3)
                        fig.update_traces(textposition='inside', textinfo='percent+label')
                        st.plotly_chart(fig, use_container_width=True)
                    else:
                        st.write("é…åˆ†ã•ã‚ŒãŸã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

            with st.container(border=True):
                st.subheader("ðŸ“… æ—¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æŽ¨ç§»")
                
                tab1, tab2, tab3 = st.tabs(["äºˆæ¸¬CVæ•° æŽ¨ç§»", "æ—¥åˆ¥ äºˆç®—é…åˆ†", "ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ¥ äºˆæ¸¬CPAæ¯”è¼ƒ"])

                with tab1:
                    daily_cv_df = pd.DataFrame([{'date': r['date'], 'cv': r['cv']} for r in daily_results if r['cv'] is not None]).set_index('date')
                    if not daily_cv_df.empty:
                        fig = px.line(daily_cv_df, x=daily_cv_df.index, y='cv', title='æ—¥åˆ¥ äºˆæ¸¬CVæ•°æŽ¨ç§»', markers=True)
                        fig.update_layout(xaxis_title='æ—¥ä»˜', yaxis_title='äºˆæ¸¬CVæ•°')
                        st.plotly_chart(fig, use_container_width=True)

                with tab2:
                    alloc_df = pd.DataFrame([res['allocation'] for res in daily_results], index=[res['date'] for res in daily_results]).fillna(0)
                    if not alloc_df.empty:
                        fig = px.bar(alloc_df, x=alloc_df.index, y=alloc_df.columns, title='æ—¥åˆ¥ äºˆç®—é…åˆ†æŽ¨ç§»',
                                     labels={'value': 'äºˆç®—ï¼ˆå††ï¼‰', 'index': 'æ—¥ä»˜', 'variable': 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³'})
                        st.plotly_chart(fig, use_container_width=True)

                with tab3:
                    cpa_df = result_df[result_df['äºˆæ¸¬CPA'] > 0].sort_values('äºˆæ¸¬CPA', ascending=True)
                    if not cpa_df.empty:
                        fig = px.bar(cpa_df, x=cpa_df.index, y='äºˆæ¸¬CPA', title='ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ¥ äºˆæ¸¬CPAæ¯”è¼ƒ',
                                     labels={'x': 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³', 'y': 'äºˆæ¸¬CPAï¼ˆå††ï¼‰'}, text_auto=True)
                        st.plotly_chart(fig, use_container_width=True)
