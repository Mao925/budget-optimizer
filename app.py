import streamlit as st
import pandas as pd
import numpy as np
import xgboost as xgb
import pulp
import jpholiday
from datetime import datetime, timedelta
import plotly.express as px
from io import BytesIO
from pytrends.request import TrendReq
import time
import collections

# --- 0. ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®šã¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ---
st.set_page_config(layout="wide", page_title="åºƒå‘Šäºˆç®—é…åˆ† æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼")

def to_excel(df):
    """ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’Excelå½¢å¼ã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›ã™ã‚‹"""
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=True, sheet_name='Sheet1')
    processed_data = output.getvalue()
    return processed_data

def get_features(trend_keywords_dict):
    """ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¾æ›¸ã‹ã‚‰ç‰¹å¾´é‡ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    return ['cost', 'log_cost', 'weekday', 'month', 'week', 'is_holiday'] + list(trend_keywords_dict.keys())

# --- Googleãƒˆãƒ¬ãƒ³ãƒ‰é–¢é€£ã®é–¢æ•° (ä¿®æ­£) ---
@st.cache_data(ttl=3600) # 1æ™‚é–“ã‚­ãƒ£ãƒƒã‚·ãƒ¥
def fetch_and_prepare_trends_data(start_date, end_date, trend_keywords_dict):
    """
    æŒ‡å®šã•ã‚ŒãŸæœŸé–“ã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€æ—¥åˆ¥ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™ã€‚
    é•·æœŸé–“ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¯ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ãŸã‚ã€180æ—¥å˜ä½ã®ãƒãƒ£ãƒ³ã‚¯ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã€‚
    """
    pytrends = TrendReq(hl='ja-JP', tz=360)
    
    all_trends_data = {}
    for category, kw_list in trend_keywords_dict.items():
        if not kw_list:
            all_trends_data[category] = pd.Series(dtype=float)
            continue

        # ãƒãƒ£ãƒ³ã‚¯ã§ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        total_df = pd.DataFrame()
        current_start = pd.to_datetime(start_date)
        end_date_dt = pd.to_datetime(end_date)
        
        while current_start <= end_date_dt:
            current_end = current_start + timedelta(days=180)
            if current_end > end_date_dt:
                current_end = end_date_dt
            
            timeframe = f'{current_start.strftime("%Y-%m-%d")} {current_end.strftime("%Y-%m-%d")}'
            
            try:
                pytrends.build_payload(kw_list, cat=0, timeframe=timeframe, geo='JP')
                time.sleep(1) # APIã¸ã®è² è·ã‚’è»½æ¸›
                interest_over_time_df = pytrends.interest_over_time()
                
                if not interest_over_time_df.empty:
                    total_df = pd.concat([total_df, interest_over_time_df])
                
            except Exception as e:
                st.warning(f"ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®ä¸€éƒ¨å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ ({category}, {timeframe}): {e}")
                pass

            current_start = current_end + timedelta(days=1)

        if not total_df.empty and 'isPartial' in total_df.columns:
            # ãƒãƒ£ãƒ³ã‚¯ã®å¢ƒç•Œã§ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹é‡è¤‡ã‚’å‰Šé™¤
            total_df = total_df[~total_df.index.duplicated(keep='first')]
            all_trends_data[category] = total_df.drop(columns='isPartial').mean(axis=1)
        else:
            all_trends_data[category] = pd.Series(dtype=float)

    # å…¨ã‚«ãƒ†ã‚´ãƒªã‚’ä¸€ã¤ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«çµåˆ
    trends_df = pd.DataFrame(all_trends_data)
    
    # å…¨ã¦ã®æ—¥ä»˜ãŒå«ã¾ã‚Œã‚‹ã‚ˆã†ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†è¨­å®š
    all_dates = pd.date_range(start=start_date, end=end_date, freq='D')
    trends_df = trends_df.reindex(all_dates)

    # æ¬ æå€¤ã‚’å‰æ–¹ãƒ»å¾Œæ–¹ã§åŸ‹ã‚ã€ãã‚Œã§ã‚‚æ®‹ã‚‹å ´åˆã¯0ã§åŸ‹ã‚ã‚‹
    trends_df = trends_df.ffill().bfill().fillna(0)
    
    return trends_df.reset_index().rename(columns={'index': 'æ—¥'})


# --- 1. ãƒ‡ãƒ¼ã‚¿å‡¦ç†é–¢æ•° ---
@st.cache_data
def preprocess_data(uploaded_file, column_mapping, training_start_date, training_end_date, trend_keywords_dict):
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰å‡¦ç†ã—ã€ç‰¹å¾´é‡ã‚’ä½œæˆã™ã‚‹"""
    if uploaded_file is None: return None, "ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", None

    df = None
    # ã•ã¾ã–ã¾ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™
    for encoding in ['utf-8-sig', 'cp932', 'utf-8', 'sjis']:
        try:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding=encoding, header=None, dtype=str)
            break
        except Exception:
            continue
    if df is None: return None, "ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚å¯¾å¿œã™ã‚‹æ–‡å­—ã‚³ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚", None

    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’è‡ªå‹•æ¤œå‡º
    header_row_index = -1
    for i, row in df.iterrows():
        if row.astype(str).str.contains('ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å').any():
            header_row_index = i
            break
    if header_row_index == -1:
        st.session_state.column_mapping_required = True
        st.session_state.raw_df_columns = df.iloc[0].tolist()
        return None, "ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ ('ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å') ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚", None

    new_header = df.iloc[header_row_index]
    df = df[header_row_index + 1:]
    df.columns = new_header
    
    # åˆè¨ˆè¡Œã‚’å‰Šé™¤
    total_row_index = df[df.iloc[:, 0].astype(str).str.contains('åˆè¨ˆ', na=False)].index
    if not total_row_index.empty:
        df = df.loc[:total_row_index[0]-1]

    df = df.rename(columns=column_mapping)
    
    required_cols = ['æ—¥', 'campaign_name', 'cost', 'conversions']
    if not all(col in df.columns for col in required_cols):
        return None, f"å¿…è¦ãªåˆ— {required_cols} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", None
        
    df_selected = df[required_cols].copy()
    
    try:
        df_selected['æ—¥'] = pd.to_datetime(df_selected['æ—¥'])
        df_selected['cost'] = pd.to_numeric(df_selected['cost'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
        df_selected['conversions'] = pd.to_numeric(df_selected['conversions'].astype(str).str.replace(',', ''), errors='coerce').fillna(0).astype(int)
    except Exception as e:
        return None, f"ãƒ‡ãƒ¼ã‚¿å‹å¤‰æ›ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}ã€‚CSVã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚", None

    # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
    df_selected['weekday'] = df_selected['æ—¥'].dt.weekday
    df_selected['month'] = df_selected['æ—¥'].dt.month
    df_selected['week'] = df_selected['æ—¥'].dt.isocalendar().week.astype(int)
    df_selected['is_holiday'] = df_selected['æ—¥'].apply(lambda x: 1 if jpholiday.is_holiday(x) else 0)
    df_selected['log_cost'] = np.log1p(df_selected['cost'])

    # Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ãƒãƒ¼ã‚¸
    try:
        trends_df = fetch_and_prepare_trends_data(training_start_date, training_end_date, trend_keywords_dict)
        df_selected = pd.merge(df_selected, trends_df, on='æ—¥', how='left')
        df_selected[list(trend_keywords_dict.keys())] = df_selected[list(trend_keywords_dict.keys())].fillna(0)
    except Exception as e:
        return None, f"Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®å–å¾—ã¾ãŸã¯ãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}", None

    # å­¦ç¿’æœŸé–“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    training_df = df_selected[(df_selected['æ—¥'] >= pd.to_datetime(training_start_date)) & (df_selected['æ—¥'] <= pd.to_datetime(training_end_date))]
    
    if training_df.empty:
        return None, "æŒ‡å®šã•ã‚ŒãŸå­¦ç¿’æœŸé–“ã«æœ‰åŠ¹ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", None

    return training_df.sort_values(by='æ—¥').reset_index(drop=True), None

# --- 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’é–¢æ•° ---
@st.cache_data
def train_models(_df, trend_keywords_dict):
    """ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã”ã¨ã«XGBoostãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã—ã€ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚‚è¿”ã™"""
    models = {}
    feature_importances = pd.DataFrame()
    features = get_features(trend_keywords_dict)
    
    campaign_names = _df['campaign_name'].unique()
    latest_date = _df['æ—¥'].max()
    
    for campaign in campaign_names:
        campaign_df = _df[_df['campaign_name'] == campaign].copy()
        if len(campaign_df) < 10: continue

        # ç›´è¿‘ã®ãƒ‡ãƒ¼ã‚¿ã«é‡ã¿ä»˜ã‘
        recency_in_days = (latest_date - campaign_df['æ—¥']).dt.days
        sample_weights = np.where(recency_in_days <= 30, 3.0, 1.0)
        
        X = campaign_df[features]
        y = campaign_df['conversions']
        
        model = xgb.XGBRegressor(
            objective='reg:squarederror', n_estimators=100, learning_rate=0.1,
            max_depth=3, subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1
        )
        model.fit(X, y, sample_weight=sample_weights)
        models[campaign] = model

        # ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’æ ¼ç´
        temp_importance = pd.DataFrame(
            data={'feature': features, 'importance': model.feature_importances_, 'campaign': campaign}
        )
        feature_importances = pd.concat([feature_importances, temp_importance], ignore_index=True)
        
    return models, feature_importances

# --- 3. æœ€é©åŒ–é–¢æ•° ---
def optimize_budget_allocation(total_budget, models, features_today, campaign_max_budgets, trend_keywords_dict):
    """æ•°ç†æœ€é©åŒ–ã‚’ç”¨ã„ã¦ã€CVã‚’æœ€å¤§åŒ–ã™ã‚‹äºˆç®—é…åˆ†ã‚’è¨ˆç®—ã™ã‚‹"""
    campaign_names = list(models.keys())
    features = get_features(trend_keywords_dict)
    problem = pulp.LpProblem("Budget_Allocation_Problem", pulp.LpMaximize)
    
    # äºˆç®—ã®åˆ»ã¿å¹…ã‚’è¨­å®š
    step = max(1000, int(total_budget / 100))
    budget_steps = list(range(0, total_budget + 1, step))
    if not budget_steps: budget_steps = [0, total_budget]
    
    choices = pulp.LpVariable.dicts("Choice", (campaign_names, budget_steps), cat='Binary')
    
    # å„ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ãƒ»å„äºˆç®—ã‚¹ãƒ†ãƒƒãƒ—ã§ã®CVæ•°ã‚’äºˆæ¸¬
    predicted_cvs = {}
    for campaign in campaign_names:
        predicted_cvs[campaign] = {}
        pred_data_list = []
        for budget in budget_steps:
            pred_data_point = {'cost': budget, 'log_cost': np.log1p(budget), **features_today}
            pred_data_list.append(pred_data_point)

        pred_df = pd.DataFrame(pred_data_list)[features]
        predictions = models[campaign].predict(pred_df)
        for i, budget in enumerate(budget_steps):
            predicted_cvs[campaign][budget] = max(0, predictions[i])

    # ç›®çš„é–¢æ•°ï¼šç·CVæ•°ã‚’æœ€å¤§åŒ–
    problem += pulp.lpSum(predicted_cvs[c][b] * choices[c][b] for c in campaign_names for b in budget_steps)
    
    # åˆ¶ç´„æ¡ä»¶
    # 1. ç·äºˆç®—ã‚’è¶…ãˆãªã„
    problem += pulp.lpSum(b * choices[c][b] for c in campaign_names for b in budget_steps) <= total_budget
    # 2. å„ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã¯1ã¤ã®äºˆç®—ã—ã‹é¸ã¹ãªã„
    for c in campaign_names:
        problem += pulp.lpSum(choices[c][b] for b in budget_steps) == 1
    # 3. å„ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®ä¸Šé™äºˆç®—ã‚’è¶…ãˆãªã„
    for c in campaign_names:
        problem += pulp.lpSum(b * choices[c][b] for b in budget_steps) <= campaign_max_budgets[c]

    problem.solve(pulp.PULP_CBC_CMD(msg=0))
    
    optimal_allocation, total_predicted_cv, cv_per_campaign = {}, None, {}
    if pulp.LpStatus[problem.status] == "Optimal":
        total_predicted_cv = pulp.value(problem.objective)
        for c in campaign_names:
            for b in budget_steps:
                if choices[c][b].varValue == 1:
                    optimal_allocation[c] = b
                    cv_per_campaign[c] = predicted_cvs[c][b]
                    break
    
    return optimal_allocation, total_predicted_cv, cv_per_campaign, pulp.LpStatus[problem.status]

# --- 4. Streamlit UI ---
st.title('ğŸš€ åºƒå‘Šäºˆç®—é…åˆ† æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚¿ãƒ¼ (v2)')

# --- session_state ã®åˆæœŸåŒ– ---
if 'data_processed' not in st.session_state:
    st.session_state.data_processed = False
if 'column_mapping_required' not in st.session_state:
    st.session_state.column_mapping_required = False
if 'trend_keywords' not in st.session_state:
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®š
    st.session_state.trend_keywords = collections.OrderedDict({
        'general_trend': ['å¡¾è¬›å¸« ãƒã‚¤ãƒˆ', 'å¡¾ ãƒã‚¤ãƒˆ'],
        'brand_trend': ['å¡¾è¬›å¸«ã‚¹ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³'],
        'school_trend': ['æ—©ç¨²ç”°ã‚¢ã‚«ãƒ‡ãƒŸãƒ¼ ãƒã‚¤ãƒˆ', 'æ­¦ç”°å¡¾ ãƒã‚¤ãƒˆ', 'ITTOå€‹åˆ¥æŒ‡å°å­¦é™¢ ãƒã‚¤ãƒˆ', 'è‡¨æµ·ã‚»ãƒŸãƒŠãƒ¼ ãƒã‚¤ãƒˆ'],
        'station_trend': ['æ±äº¬ å¡¾ æ±‚äºº', 'å¤§é˜ª å¡¾ æ±‚äºº', 'åå¤å±‹ å¡¾ æ±‚äºº', 'äº¬éƒ½ å¡¾ æ±‚äºº']
    })

# --- ã‚µã‚¤ãƒ‰ãƒãƒ¼ ---
with st.sidebar:
    st.header('âš™ï¸ åŸºæœ¬è¨­å®š')
    uploaded_file = st.file_uploader("â‘  ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=['csv'])

    if uploaded_file:
        # å­¦ç¿’æœŸé–“ã®è‡ªå‹•è¨­å®š
        default_start_date = datetime.now().date() - timedelta(days=90)
        default_end_date = datetime.now().date()
        try:
            uploaded_file.seek(0)
            temp_df = pd.read_csv(uploaded_file, encoding='cp932', on_bad_lines='skip')
            date_col = next((col for col in temp_df.columns if temp_df[col].astype(str).str.match(r'\d{4}[/-]\d{1,2}[/-]\d{1,2}').any()), None)
            if date_col:
                dates = pd.to_datetime(temp_df[date_col], errors='coerce').dropna()
                if not dates.empty:
                    default_start_date, default_end_date = dates.min().date(), dates.max().date()
        except Exception:
            pass # ã‚¨ãƒ©ãƒ¼ã§ã‚‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã§ç¶šè¡Œ
        finally:
            uploaded_file.seek(0)

        st.subheader("â‘¡ å­¦ç¿’ãƒ‡ãƒ¼ã‚¿æœŸé–“")
        training_start_date = st.date_input('é–‹å§‹æ—¥', value=default_start_date)
        training_end_date = st.date_input('çµ‚äº†æ—¥', value=default_end_date)
    
    # ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    with st.expander("â‘¢ Googleãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š (æ¨å¥¨)", expanded=False):
        st.info("äºˆæ¸¬ç²¾åº¦å‘ä¸Šã®ãŸã‚ã€é–¢é€£ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã‚«ãƒ†ã‚´ãƒªã”ã¨ã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šå¯èƒ½ã§ã™ã€‚")
        
        temp_keywords = {}
        for category, keywords in st.session_state.trend_keywords.items():
            input_str = st.text_area(f"ã‚«ãƒ†ã‚´ãƒª: {category}", ", ".join(keywords), height=50)
            temp_keywords[category] = [kw.strip() for kw in input_str.split(',') if kw.strip()]
        
        if st.button("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ›´æ–°"):
            st.session_state.trend_keywords = collections.OrderedDict(temp_keywords)
            st.success("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚")

    if uploaded_file:
        process_button = st.button('ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã™ã‚‹', type="primary", use_container_width=True)
    else:
        process_button = False

# --- ãƒ¡ã‚¤ãƒ³ç”»é¢ ---
if uploaded_file is None:
    st.info("ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦é–‹å§‹ã—ã¦ãã ã•ã„ã€‚")
else:
    # åˆ—åãƒãƒƒãƒ”ãƒ³ã‚°å‡¦ç†
    column_mapping = {'æ—¥': 'æ—¥', 'ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å': 'campaign_name', 'ã‚³ã‚¹ãƒˆ': 'cost', 'ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•°': 'conversions'}
    if st.session_state.column_mapping_required:
        with st.expander("âš ï¸ åˆ—åã®ãƒãƒƒãƒ”ãƒ³ã‚°ãŒå¿…è¦ã§ã™", expanded=True):
            st.warning("CSVã®åˆ—åã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã®ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰å¯¾å¿œã™ã‚‹åˆ—åã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")
            raw_cols = st.session_state.raw_df_columns
            user_mapping = {}
            for jp_col, en_col in column_mapping.items():
                selected_col = st.selectbox(f"ã€Œ{jp_col}ã€ã«å¯¾å¿œã™ã‚‹åˆ—", [None] + raw_cols)
                if selected_col:
                    user_mapping[selected_col] = en_col
            column_mapping = {k: v for k, v in user_mapping.items() if k is not None}

    if process_button:
        with st.spinner('ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨Googleãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿å–å¾—ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®Ÿè¡Œä¸­...'):
            data, error_message = preprocess_data(uploaded_file, column_mapping, training_start_date, training_end_date, st.session_state.trend_keywords)
            if error_message:
                st.error(f"ã‚¨ãƒ©ãƒ¼: {error_message}")
                st.session_state.data_processed = False
            else:
                st.session_state.original_data = data
                models, importances = train_models(data, st.session_state.trend_keywords)
                st.session_state.trained_models = models
                st.session_state.feature_importances = importances
                st.session_state.data_processed = True
                st.success("ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    if st.session_state.data_processed:
        st.header("â‘£ æœ€é©åŒ–ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³")
        
        col1, col2 = st.columns(2)
        with col1:
            optim_start_date = st.date_input('æœ€é©åŒ–æœŸé–“ï¼ˆé–‹å§‹æ—¥ï¼‰', value=datetime.now().date() + timedelta(days=1))
        with col2:
            optim_end_date = st.date_input('æœ€é©åŒ–æœŸé–“ï¼ˆçµ‚äº†æ—¥ï¼‰', value=datetime.now().date() + timedelta(days=7))

        if optim_start_date > optim_end_date:
            st.error('ã‚¨ãƒ©ãƒ¼: çµ‚äº†æ—¥ã¯é–‹å§‹æ—¥ä»¥é™ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚')
        else:
            st.subheader("ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–è¨­å®š")
            total_daily_budget = st.slider('1æ—¥ã‚ãŸã‚Šã®ç·äºˆç®—ï¼ˆå††ï¼‰', min_value=10000, max_value=500000, step=1000, value=100000)
            
            with st.expander("ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ¥ã®ä¸Šé™äºˆç®—ã‚’è¨­å®šã™ã‚‹ï¼ˆæ¨å¥¨ï¼‰"):
                st.info("ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³æ¯ã«æ¶ˆåŒ–å¯èƒ½ãªä¸Šé™äºˆç®—ã‚’ã‚¹ãƒ©ã‚¤ãƒ€ãƒ¼ã§è¨­å®šã™ã‚‹ã¨ã€ã‚ˆã‚Šç¾å®Ÿçš„ãªé…åˆ†ã«ãªã‚Šã¾ã™ã€‚")
                campaign_max_budgets_input = {}
                for campaign in st.session_state.trained_models.keys():
                    campaign_max_budgets_input[campaign] = st.slider(f"ã€{campaign}ã€‘ã®ä¸Šé™äºˆç®—ï¼ˆå††/æ—¥ï¼‰", 0, total_daily_budget, total_daily_budget, 500)

            run_optimization_button = st.button('ğŸš€ ã“ã®è¨­å®šã§æœ€é©åŒ–ã‚’å®Ÿè¡Œã™ã‚‹', type="primary", use_container_width=True)

            if run_optimization_button:
                with st.spinner('æœ€é©åŒ–è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...'):
                    # æœ€é©åŒ–æœŸé–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
                    trends_for_optim = fetch_and_prepare_trends_data(optim_start_date, optim_end_date, st.session_state.trend_keywords)
                    trends_for_optim.set_index('æ—¥', inplace=True)

                    date_range = pd.date_range(optim_start_date, optim_end_date)
                    daily_results = []
                    progress_bar = st.progress(0, text="æœ€é©åŒ–è¨ˆç®—ã‚’å®Ÿè¡Œä¸­...")

                    for i, target_date in enumerate(date_range):
                        target_date_ts = pd.Timestamp(target_date)
                        trend_date_to_use = min(target_date_ts, trends_for_optim.index.max())
                        
                        features_for_today = {
                            'weekday': target_date.weekday(), 'month': target_date.month,
                            'week': target_date.isocalendar().week,
                            'is_holiday': 1 if jpholiday.is_holiday(target_date) else 0,
                            **trends_for_optim.loc[trend_date_to_use].to_dict()
                        }

                        optimal_budgets, total_cv, cv_breakdown, status = optimize_budget_allocation(
                            total_daily_budget, st.session_state.trained_models, features_for_today, 
                            campaign_max_budgets_input, st.session_state.trend_keywords
                        )
                        daily_results.append({'date': target_date, 'allocation': optimal_budgets, 'cv': total_cv, 'cv_breakdown': cv_breakdown, 'status': status})
                        progress_bar.progress((i + 1) / len(date_range), text=f"æœ€é©åŒ–è¨ˆç®—: {target_date.strftime('%m/%d')}")
                    
                    st.session_state.daily_results = daily_results
                    st.session_state.optim_period = (optim_start_date, optim_end_date)
                    st.success("æœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")

        # --- çµæœè¡¨ç¤º ---
        if 'daily_results' in st.session_state and st.session_state.daily_results:
            st.header(f'ğŸ“Š æœ€é©åŒ–çµæœï¼ˆ{st.session_state.optim_period[0].strftime("%Y/%m/%d")} ã€œ {st.session_state.optim_period[1].strftime("%Y/%m/%d")}ï¼‰')
            
            daily_results = st.session_state.daily_results
            # 1æ—¥ã§ã‚‚æœ€é©åŒ–ãŒå¤±æ•—ã—ãŸã‹ãƒã‚§ãƒƒã‚¯
            if any(res['status'] != "Optimal" for res in daily_results):
                failed_dates = [res['date'].strftime('%Y-%m-%d') for res in daily_results if res['status'] != "Optimal"]
                st.error(f"ä»¥ä¸‹ã®æ—¥ä»˜ã§æœ€é©åŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {', '.join(failed_dates)}\n\n"
                         f"**è€ƒãˆã‚‰ã‚Œã‚‹åŸå› ã¨å¯¾ç­–:**\n"
                         f"- è¨­å®šã•ã‚ŒãŸã€Œ1æ—¥ã‚ãŸã‚Šã®ç·äºˆç®—ã€ãŒä½ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n"
                         f"- ã€Œã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ¥ã®ä¸Šé™äºˆç®—ã€ã®åˆ¶ç´„ãŒå³ã—ã™ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n\n"
                         f"äºˆç®—è¨­å®šã‚’è¦‹ç›´ã—ã¦ã€å†åº¦æœ€é©åŒ–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
            
            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼
            with st.container(border=True):
                st.subheader("ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼")
                total_allocated_sum = sum(sum(res['allocation'].values()) for res in daily_results if res['allocation'])
                total_predicted_cv_sum = sum(res['cv'] for res in daily_results if res['cv'] is not None)
                total_predicted_cpa = total_allocated_sum / total_predicted_cv_sum if total_predicted_cv_sum > 0 else 0
                
                col1, col2, col3 = st.columns(3)
                col1.metric(label="äºˆæ¸¬ é…åˆ†åˆè¨ˆé‡‘é¡", value=f"{total_allocated_sum:,.0f} å††")
                col2.metric(label="äºˆæ¸¬ ç·CVæ•°", value=f"{total_predicted_cv_sum:.2f} ä»¶")
                col3.metric(label="äºˆæ¸¬ CPA", value=f"{total_predicted_cpa:,.0f} å††")

            # çµæœè©³ç´°
            avg_allocations = pd.DataFrame([res['allocation'] for res in daily_results if res['allocation']]).mean()
            avg_cvs = pd.DataFrame([res['cv_breakdown'] for res in daily_results if res['cv_breakdown']]).mean()
            result_df = pd.DataFrame({'1æ—¥ã‚ãŸã‚Šã®å¹³å‡æ¨å¥¨äºˆç®—': avg_allocations, '1æ—¥ã‚ãŸã‚Šã®å¹³å‡äºˆæ¸¬CVæ•°': avg_cvs}).fillna(0)
            result_df['äºˆæ¸¬CPA'] = (result_df['1æ—¥ã‚ãŸã‚Šã®å¹³å‡æ¨å¥¨äºˆç®—'] / result_df['1æ—¥ã‚ãŸã‚Šã®å¹³å‡äºˆæ¸¬CVæ•°']).replace([np.inf, -np.inf], 0).fillna(0)
            result_df = result_df.round(2).astype({'1æ—¥ã‚ãŸã‚Šã®å¹³å‡æ¨å¥¨äºˆç®—': int, 'äºˆæ¸¬CPA': int})
            
            tab1, tab2, tab3 = st.tabs(["ã‚µãƒãƒªãƒ¼ & æ¨ç§»", "æ—¥åˆ¥è©³ç´°ãƒ‡ãƒ¼ã‚¿", "äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã®è¨ºæ–­æƒ…å ±"])

            with tab1:
                col1, col2 = st.columns([0.6, 0.4])
                with col1:
                    st.subheader("ğŸ“‹ ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆæ—¥å¹³å‡ï¼‰")
                    st.dataframe(result_df, use_container_width=True)
                    st.download_button(
                        label="è©³ç´°çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ (Excel)", data=to_excel(result_df),
                        file_name=f"optimization_result_{datetime.now().strftime('%Y%m%d')}.xlsx",
                        mime="application/vnd.ms-excel", use_container_width=True
                    )
                with col2:
                    st.subheader("ğŸ° äºˆç®—é…åˆ†æ¯”ç‡ï¼ˆæ—¥å¹³å‡ï¼‰")
                    plot_df = result_df[result_df['1æ—¥ã‚ãŸã‚Šã®å¹³å‡æ¨å¥¨äºˆç®—'] > 0]
                    if not plot_df.empty:
                        fig_pie = px.pie(plot_df, values='1æ—¥ã‚ãŸã‚Šã®å¹³å‡æ¨å¥¨äºˆç®—', names=plot_df.index, hole=.3)
                        fig_pie.update_traces(textposition='inside', textinfo='percent+label')
                        st.plotly_chart(fig_pie, use_container_width=True)

                st.subheader("ğŸ“… æ—¥åˆ¥ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¨ç§»")
                daily_cv_df = pd.DataFrame([{'date': r['date'], 'cv': r['cv']} for r in daily_results if r['cv'] is not None]).set_index('date')
                if not daily_cv_df.empty:
                    fig_line = px.line(daily_cv_df, x=daily_cv_df.index, y='cv', title='æ—¥åˆ¥ äºˆæ¸¬CVæ•°æ¨ç§»', markers=True)
                    fig_line.update_layout(xaxis_title='æ—¥ä»˜', yaxis_title='äºˆæ¸¬CVæ•°')
                    st.plotly_chart(fig_line, use_container_width=True)

            with tab2:
                st.subheader("ğŸ“‹ æ—¥åˆ¥ äºˆç®—é…åˆ†è©³ç´°")
                alloc_df = pd.DataFrame([res['allocation'] for res in daily_results], index=[res['date'].strftime('%Y-%m-%d') for res in daily_results]).fillna(0).astype(int)
                st.dataframe(alloc_df, use_container_width=True)

            with tab3:
                st.subheader("ğŸ©º ç‰¹å¾´é‡ã®é‡è¦åº¦")
                st.info("ã“ã‚Œã¯ã€ã©ã®è¦ç´ ï¼ˆæ›œæ—¥ã€æœˆã€ç¥æ—¥ã€ãƒˆãƒ¬ãƒ³ãƒ‰ãªã©ï¼‰ãŒCVæ•°äºˆæ¸¬ã«å½±éŸ¿ã‚’ä¸ãˆãŸã‹ã‚’ç¤ºã™æŒ‡æ¨™ã§ã™ã€‚æœ€é©åŒ–çµæœã®ã€Œå¥å…¨æ€§ã€ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®è¨ºæ–­æ©Ÿèƒ½ã¨ã—ã¦ã”æ´»ç”¨ãã ã•ã„ã€‚")
                
                importances = st.session_state.get('feature_importances')
                if importances is not None and not importances.empty:
                    # ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã”ã¨ã«å¹³å‡ã‚’å–ã‚‹
                    avg_importances = importances.groupby('feature')['importance'].mean().sort_values(ascending=False)
                    fig_imp = px.bar(avg_importances, x=avg_importances.values, y=avg_importances.index, orientation='h', title='ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆå…¨ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³å¹³å‡ï¼‰')
                    fig_imp.update_layout(xaxis_title='é‡è¦åº¦', yaxis_title='ç‰¹å¾´é‡')
                    st.plotly_chart(fig_imp, use_container_width=True)
